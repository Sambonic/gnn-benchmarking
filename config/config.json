{
    "PROJECT_NAME": "GNN Benchmarking",
    "GITHUB_USERNAME": "Sambonic",
    "REPO_NAME": "gnn-benchmarking",
    "PROJECT_DESCRIPTION": "Comparing between the performance of different GNNs on the same dataset",
    "PROJECT_FEATURES": "- **Reddit Post Classification:**  Classifies Reddit posts into subreddits using graph neural networks (GNNs).\n- **Heterogeneous Graph Construction:** Creates a heterogeneous graph representing posts, authors, and relationships between them (e.g., authorship, post similarity).\n- **Text Embedding:** Uses SentenceTransformer ('all-MiniLM-L6-v2') to generate embeddings for post content.\n- **GNN Model Benchmarking:** Trains and evaluates multiple GNN architectures (GraphSAGE, GatedGraphConv, GCN, GIN) on the constructed graph.\n- **Hyperparameter Tuning:**  Experiments with different hyperparameters for each GNN model to find optimal settings.\n- **Performance Evaluation:**  Assesses model performance using metrics like accuracy, precision, recall, and F1-score, along with visualization of learning curves and confusion matrices.\n- **Resource Monitoring:** Tracks and reports training time and peak memory usage for each model.\n- **TensorBoard Integration:** Uses TensorBoard for visualization and logging of training progress, metrics, and model graphs.\n\n",
    "PROJECT_USAGE": "## GNN Model Benchmarking Project Walkthrough\n\nThis project benchmarks four different Graph Neural Network (GNN) models on a Reddit dataset.  The steps below detail the process, assuming you've already cloned the repository and installed the necessary libraries.\n\n**1. Data Loading and Preprocessing:**\n\nThe notebook begins by loading the `webis/tldr-17` dataset from Hugging Face.  This dataset contains Reddit posts, including their content, authors, subreddits, and more. The code then converts the Hugging Face dataset to a Pandas DataFrame for easier manipulation.\n\n```python\n# load the reddit dataset from HuggingFace repositories\nds = load_dataset(\n    \"webis/tldr-17\",\n    trust_remote_code=True,\n    split=\"train[:100%]\",)\n\n# convert the HuggingFace dataset to pandas for easier use\ndf = ds.to_pandas()\n```\n\nNext, it filters the dataset to include only the top 3 subreddits and downsamples the data to balance the class distribution.  Text cleaning is performed using a custom function `clean_text`, which removes digits, non-word characters, single-character words, and stop words.\n\n```python\n# ... (subreddit filtering and downsampling code) ...\n\ndef clean_text(text):\n  # ... (text cleaning logic) ...\n\ndf[\"content\"] = df[\"content\"].apply(clean_text)\n```\n\n**2. Text Embedding and Feature Encoding:**\n\nSentence embeddings are generated using the `all-MiniLM-L6-v2` SentenceTransformer model. These embeddings represent the semantic meaning of the post content.  Subreddits, authors, and post IDs are then label encoded to numerical representations.  Unnecessary columns are dropped from the DataFrame.\n\n```python\n# use Microsoft's all-MiniLM-L6-v2 model for text embedding\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nmodel = model.to('cuda') # move the model to GPU if available\nembeddings = model.encode(df['content'].tolist(), device='cuda')\ndf['content'] = list(embeddings)\n\n# ... (Label encoding for subreddits, authors, and post IDs) ...\n```\n\n**3. Graph Creation:**\n\nA heterogeneous graph is constructed using `torch_geometric.data.HeteroData`.  The graph consists of:\n\n* **Nodes:**  `post` nodes (with content embeddings as features and subreddit as labels), and `author` nodes (with placeholder features).\n* **Edges:** `author`-to-`post` edges representing authorship, and `post`-to-`post` edges representing similarity between posts (based on cosine similarity of their embeddings). A threshold is used to determine which pairs of posts are considered similar.\n\n```python\ndata = HeteroData()\n# ... (Adding post nodes, author nodes, and edges) ...\n```\nA visualization of a smaller subgraph is created using `networkx` to illustrate the graph structure.\n\n\n**4. Data Splitting and Masking:**\n\nThe dataset is split into training, validation, and testing sets using stratified sampling to maintain class proportions.  Boolean masks are created to easily select the relevant data for each set.\n\n```python\n# ... (train_test_split and mask creation code) ...\n```\n\n\n**5. Model Definition and Training:**\n\nFour different GNN models are defined: `GraphSAGE`, `GatedGraphConv`, `GCN`, and `GIN`. These models use different architectures to process the graph data and predict the subreddit of a post.  The `train_model` function trains each model using the Adam optimizer and cross-entropy loss, logging training progress to TensorBoard.\n\n```python\nclass GraphSAGE(torch.nn.Module):\n    # ... (GraphSAGE model definition) ...\n\nclass GatedGNN(torch.nn.Module):\n    # ... (GatedGNN model definition) ...\n\nclass GCN(torch.nn.Module):\n    # ... (GCN model definition) ...\n\nclass GIN(torch.nn.Module):\n    # ... (GIN model definition) ...\n\n# ... (Training loop using train_model function) ...\n```\n\n**6. Model Evaluation:**\n\nThe `test_model` function evaluates each trained model on the test set, calculating accuracy, precision, recall, and F1-score.  The results are logged to TensorBoard.  The notebook also generates various plots to visualize the training progress and model performance comparison.  A grouped bar chart compares the evaluation metrics for all the models and another chart shows the training time and memory consumption of the models.  Finally, the confusion matrices are plotted for each model to visualize the classification performance.\n\n```python\ndef test_model(model, X_test, y_test, edge_index, log_dir='./runs'):\n    # ... (Testing and metric calculation code) ...\n\n\n# ... (Plotting code) ...\n```\n\n**7. Hyperparameter Tuning:**\n\nA section is dedicated to hyperparameter tuning where a dictionary `test_dict` specifies the hyperparameters to test and the `test_hyperparams` functions iterate through all combinations using the updated `train_model` function that saves results per hyperparameter combination to TensorBoard.\n\n\n**To run this project:**\n\n1. **Ensure you have a compatible environment set up:**  This project requires Python with the libraries specified in the `%pip install` line at the beginning. A GPU is recommended but not strictly required.\n2. **Clone the repository and install the required libraries.** (Instructions not included here as requested)\n3. **Run the Jupyter Notebook:** Execute each code cell sequentially.  The TensorBoard visualizations will be available after training is complete.  Open TensorBoard by running `%tensorboard --logdir runs` in a Jupyter Notebook cell (or from your command line: `tensorboard --logdir runs`).\n\n\nThis detailed walkthrough provides a comprehensive understanding of the project's functionality and execution.  Remember that the training process may take a considerable amount of time depending on your hardware.\n"
}